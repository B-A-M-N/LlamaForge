#!/usr/bin/env python3
"""
Download real datasets to close gaps and reach Claude 3.5-like targets.
"""
import json
import os
from datasets import load_dataset
from tqdm import tqdm

def ensure_dir(path):
    """Ensure directory exists."""
    os.makedirs(path, exist_ok=True)

def convert_to_instruction_format(example, dataset_name):
    """Convert various dataset formats to instruction format."""
    result = {
        '_source': dataset_name,
        '_category': 'unknown'
    }

    # Handle different formats
    if 'question' in example and 'answer' in example:
        result['instruction'] = example['question']
        result['input'] = ''
        result['output'] = example['answer'] if isinstance(example['answer'], str) else str(example['answer'])
    elif 'problem' in example and 'solution' in example:
        result['instruction'] = example['problem']
        result['input'] = ''
        result['output'] = example['solution']
    elif 'query' in example and 'response' in example:
        result['instruction'] = example['query']
        result['input'] = ''
        result['output'] = example['response']
    elif 'text' in example:
        result['instruction'] = example['text']
        result['input'] = ''
        result['output'] = example.get('label', '')
    elif 'prompt' in example and 'completion' in example:
        result['instruction'] = example['prompt']
        result['input'] = ''
        result['output'] = example['completion']
    else:
        # Try to infer from keys
        keys = list(example.keys())
        if len(keys) >= 2:
            result['instruction'] = str(example[keys[0]])
            result['input'] = ''
            result['output'] = str(example[keys[1]])

    return result

def download_reasoning_datasets():
    """Download reasoning and CoT datasets."""
    print("\n" + "="*80)
    print("üß† DOWNLOADING REASONING/CoT DATASETS")
    print("="*80)

    base_path = '/home/joker/LlamaForge/examples/datasets/gap_spanning/reasoning'
    ensure_dir(base_path)

    datasets_to_download = [
        {
            'name': 'hendrycks/competition_math',
            'split': 'train',
            'output': 'math_dataset.jsonl',
            'category': 'reasoning_trace',
            'limit': None
        },
        {
            'name': 'meta-math/MetaMathQA',
            'split': 'train',
            'output': 'metamath_qa.jsonl',
            'category': 'reasoning_trace',
            'limit': 200000  # Sample 200k from 395k
        },
        {
            'name': 'tau/commonsense_qa',
            'split': 'train',
            'output': 'commonsense_qa_extended.jsonl',
            'category': 'reasoning_trace',
            'limit': None
        },
        {
            'name': 'TIGER-Lab/TheoremQA',
            'split': 'train',
            'output': 'theorem_qa.jsonl',
            'category': 'reasoning_trace',
            'limit': None
        },
        {
            'name': 'aqua_rat',
            'split': 'train',
            'output': 'aqua_rat.jsonl',
            'category': 'reasoning_trace',
            'limit': None
        },
    ]

    total_downloaded = 0

    for ds_info in datasets_to_download:
        try:
            print(f"\nüì• Downloading {ds_info['name']}...")

            dataset = load_dataset(ds_info['name'], split=ds_info['split'], trust_remote_code=True)

            if ds_info['limit'] and len(dataset) > ds_info['limit']:
                dataset = dataset.shuffle(seed=42).select(range(ds_info['limit']))

            output_path = os.path.join(base_path, ds_info['output'])
            count = 0

            with open(output_path, 'w') as f:
                for example in tqdm(dataset, desc=f"Processing {ds_info['name']}"):
                    try:
                        converted = convert_to_instruction_format(example, ds_info['name'])
                        converted['_category'] = ds_info['category']
                        f.write(json.dumps(converted) + '\n')
                        count += 1
                    except Exception as e:
                        continue

            print(f"‚úì Saved {count:,} examples to {ds_info['output']}")
            total_downloaded += count

        except Exception as e:
            print(f"‚ö†Ô∏è  Error downloading {ds_info['name']}: {e}")

    print(f"\n‚úÖ Total reasoning examples downloaded: {total_downloaded:,}")
    return total_downloaded

def download_code_datasets():
    """Download code and programming datasets."""
    print("\n" + "="*80)
    print("üíª DOWNLOADING CODE DATASETS")
    print("="*80)

    base_path = '/home/joker/LlamaForge/examples/datasets/gap_spanning/code'
    ensure_dir(base_path)

    datasets_to_download = [
        {
            'name': 'deepmind/code_contests',
            'split': 'train',
            'output': 'code_contests.jsonl',
            'category': 'code_instruction_multilang',
            'limit': None
        },
        {
            'name': 'codeparrot/apps',
            'split': 'train',
            'output': 'apps_full.jsonl',
            'category': 'code_instruction_multilang',
            'limit': None
        },
        {
            'name': 'code_x_glue_cc_code_completion_token',
            'subset': 'python',
            'split': 'train',
            'output': 'code_completion_python.jsonl',
            'category': 'code_instruction_multilang',
            'limit': 50000
        },
    ]

    total_downloaded = 0

    for ds_info in datasets_to_download:
        try:
            print(f"\nüì• Downloading {ds_info['name']}...")

            if 'subset' in ds_info:
                dataset = load_dataset(ds_info['name'], ds_info['subset'], split=ds_info['split'], trust_remote_code=True)
            else:
                dataset = load_dataset(ds_info['name'], split=ds_info['split'], trust_remote_code=True)

            if ds_info['limit'] and len(dataset) > ds_info['limit']:
                dataset = dataset.shuffle(seed=42).select(range(ds_info['limit']))

            output_path = os.path.join(base_path, ds_info['output'])
            count = 0

            with open(output_path, 'w') as f:
                for example in tqdm(dataset, desc=f"Processing {ds_info['name']}"):
                    try:
                        converted = convert_to_instruction_format(example, ds_info['name'])
                        converted['_category'] = ds_info['category']
                        f.write(json.dumps(converted) + '\n')
                        count += 1
                    except Exception as e:
                        continue

            print(f"‚úì Saved {count:,} examples to {ds_info['output']}")
            total_downloaded += count

        except Exception as e:
            print(f"‚ö†Ô∏è  Error downloading {ds_info['name']}: {e}")

    print(f"\n‚úÖ Total code examples downloaded: {total_downloaded:,}")
    return total_downloaded

def download_factual_datasets():
    """Download factual QA datasets."""
    print("\n" + "="*80)
    print("üìö DOWNLOADING FACTUAL QA DATASETS")
    print("="*80)

    base_path = '/home/joker/LlamaForge/examples/datasets/gap_spanning/factual'
    ensure_dir(base_path)

    datasets_to_download = [
        {
            'name': 'trivia_qa',
            'subset': 'unfiltered',
            'split': 'train',
            'output': 'trivia_qa.jsonl',
            'category': 'factual_grounding',
            'limit': 95000
        },
        {
            'name': 'hotpot_qa',
            'subset': 'fullwiki',
            'split': 'train',
            'output': 'hotpot_qa.jsonl',
            'category': 'factual_grounding',
            'limit': 90000
        },
        {
            'name': 'cais/mmlu',
            'split': 'test',
            'output': 'mmlu.jsonl',
            'category': 'factual_grounding',
            'limit': None
        },
    ]

    total_downloaded = 0

    for ds_info in datasets_to_download:
        try:
            print(f"\nüì• Downloading {ds_info['name']}...")

            if 'subset' in ds_info:
                dataset = load_dataset(ds_info['name'], ds_info['subset'], split=ds_info['split'], trust_remote_code=True)
            else:
                dataset = load_dataset(ds_info['name'], split=ds_info['split'], trust_remote_code=True)

            if ds_info['limit'] and len(dataset) > ds_info['limit']:
                dataset = dataset.shuffle(seed=42).select(range(ds_info['limit']))

            output_path = os.path.join(base_path, ds_info['output'])
            count = 0

            with open(output_path, 'w') as f:
                for example in tqdm(dataset, desc=f"Processing {ds_info['name']}"):
                    try:
                        converted = convert_to_instruction_format(example, ds_info['name'])
                        converted['_category'] = ds_info['category']
                        f.write(json.dumps(converted) + '\n')
                        count += 1
                    except Exception as e:
                        continue

            print(f"‚úì Saved {count:,} examples to {ds_info['output']}")
            total_downloaded += count

        except Exception as e:
            print(f"‚ö†Ô∏è  Error downloading {ds_info['name']}: {e}")

    print(f"\n‚úÖ Total factual examples downloaded: {total_downloaded:,}")
    return total_downloaded

def download_multiturn_datasets():
    """Download multi-turn conversation datasets."""
    print("\n" + "="*80)
    print("üí¨ DOWNLOADING MULTI-TURN DIALOG DATASETS")
    print("="*80)

    base_path = '/home/joker/LlamaForge/examples/datasets/gap_spanning/multiturn'
    ensure_dir(base_path)

    datasets_to_download = [
        {
            'name': 'openchat/openchat_sharegpt4_dataset',
            'split': 'train',
            'output': 'sharegpt_cleaned.jsonl',
            'category': 'multiturn_dialog',
            'limit': 90000
        },
        {
            'name': 'HuggingFaceH4/ultrachat_200k',
            'split': 'train_sft',
            'output': 'ultrachat_additional.jsonl',
            'category': 'multiturn_dialog',
            'limit': 100000
        },
    ]

    total_downloaded = 0

    for ds_info in datasets_to_download:
        try:
            print(f"\nüì• Downloading {ds_info['name']}...")

            dataset = load_dataset(ds_info['name'], split=ds_info['split'], trust_remote_code=True)

            if ds_info['limit'] and len(dataset) > ds_info['limit']:
                dataset = dataset.shuffle(seed=42).select(range(ds_info['limit']))

            output_path = os.path.join(base_path, ds_info['output'])
            count = 0

            with open(output_path, 'w') as f:
                for example in tqdm(dataset, desc=f"Processing {ds_info['name']}"):
                    try:
                        converted = convert_to_instruction_format(example, ds_info['name'])
                        converted['_category'] = ds_info['category']
                        f.write(json.dumps(converted) + '\n')
                        count += 1
                    except Exception as e:
                        continue

            print(f"‚úì Saved {count:,} examples to {ds_info['output']}")
            total_downloaded += count

        except Exception as e:
            print(f"‚ö†Ô∏è  Error downloading {ds_info['name']}: {e}")

    print(f"\n‚úÖ Total multi-turn examples downloaded: {total_downloaded:,}")
    return total_downloaded

def download_dpo_datasets():
    """Download DPO/preference datasets."""
    print("\n" + "="*80)
    print("üéØ DOWNLOADING DPO/PREFERENCE DATASETS")
    print("="*80)

    base_path = '/home/joker/LlamaForge/examples/datasets/gap_spanning/dpo'
    ensure_dir(base_path)

    datasets_to_download = [
        {
            'name': 'Anthropic/hh-rlhf',
            'split': 'train',
            'output': 'hh_rlhf_dpo.jsonl',
            'category': 'red_team',
            'limit': 100000
        },
        {
            'name': 'OpenAssistant/oasst1',
            'split': 'train',
            'output': 'openassistant.jsonl',
            'category': 'multiturn_dialog',
            'limit': 88000
        },
    ]

    total_downloaded = 0

    for ds_info in datasets_to_download:
        try:
            print(f"\nüì• Downloading {ds_info['name']}...")

            dataset = load_dataset(ds_info['name'], split=ds_info['split'], trust_remote_code=True)

            if ds_info['limit'] and len(dataset) > ds_info['limit']:
                dataset = dataset.shuffle(seed=42).select(range(ds_info['limit']))

            output_path = os.path.join(base_path, ds_info['output'])
            count = 0

            with open(output_path, 'w') as f:
                for example in tqdm(dataset, desc=f"Processing {ds_info['name']}"):
                    try:
                        converted = convert_to_instruction_format(example, ds_info['name'])
                        converted['_category'] = ds_info['category']
                        # Mark as DPO data
                        converted['_is_dpo'] = True
                        f.write(json.dumps(converted) + '\n')
                        count += 1
                    except Exception as e:
                        continue

            print(f"‚úì Saved {count:,} examples to {ds_info['output']}")
            total_downloaded += count

        except Exception as e:
            print(f"‚ö†Ô∏è  Error downloading {ds_info['name']}: {e}")

    print(f"\n‚úÖ Total DPO examples downloaded: {total_downloaded:,}")
    return total_downloaded

def main():
    print("="*80)
    print("GAP-SPANNING DATASET DOWNLOADER")
    print("="*80)
    print("Downloading real datasets to reach Claude 3.5-like targets...")

    stats = {}

    # Download all categories
    stats['reasoning'] = download_reasoning_datasets()
    stats['code'] = download_code_datasets()
    stats['factual'] = download_factual_datasets()
    stats['multiturn'] = download_multiturn_datasets()
    stats['dpo'] = download_dpo_datasets()

    print("\n" + "="*80)
    print("DOWNLOAD COMPLETE - SUMMARY")
    print("="*80)

    total = sum(stats.values())

    for category, count in stats.items():
        print(f"{category.capitalize():.<30} {count:>10,} examples")

    print("-"*80)
    print(f"{'TOTAL':.<30} {total:>10,} examples")

    print("\n‚úÖ All gap-spanning datasets downloaded successfully!")
    print(f"üìÅ Location: examples/datasets/gap_spanning/")

if __name__ == '__main__':
    main()
